{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGihjEIsXZvk",
        "outputId": "484dbe1a-9de3-41dc-9403-562c677fecc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, log_loss,f1_score,accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import mlflow\n",
        "#import mlflow.sklearn\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCuvoRpGNGz_"
      },
      "outputs": [],
      "source": [
        "#! pip install mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMr7b89PpsVL",
        "outputId": "7723151d-66db-44cd-b227-fd6aaf4fdcad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM1-lSoLoXYr"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/quora/maintraindf.csv'\n",
        "test_path = '/content/drive/MyDrive/quora/maintestdf.csv'\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GRPAeZ04rC3"
      },
      "outputs": [],
      "source": [
        "train_df['clean_text1'] = train_df['clean_text1'].astype(str)\n",
        "train_df['clean_text2'] = train_df['clean_text2'].astype(str)\n",
        "\n",
        "test_df['clean_text1'] = test_df['clean_text1'].astype(str)\n",
        "test_df['clean_text2'] = test_df['clean_text2'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUTmFvNj4Tnl"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_df['question1_token'] = train_df.clean_text1.apply(word_tokenize)\n",
        "train_df['question2_token'] = train_df.clean_text2.apply(word_tokenize)\n",
        "\n",
        "\n",
        "test_df['question1_token'] = test_df.clean_text1.apply(word_tokenize)\n",
        "test_df['question2_token'] = test_df.clean_text2.apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCjqJzGwjN1u",
        "outputId": "bb72cbbf-c9da-453d-e3ee-20ea7ef3d73c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(323432, 36)\n",
            "(80858, 36)\n",
            "Index(['percentage_common_tokens', 'question1_length', 'question2_length',\n",
            "       'length_difference', 'num_capital_letters1', 'num_capital_letters2',\n",
            "       'num_question_marks1', 'num_question_marks2', 'starts_with_are',\n",
            "       'starts_with_can', 'starts_with_how', 'clean_text1', 'clean_text2',\n",
            "       'word_count1', 'word_count2', 'sentence_count1', 'sentence_count2',\n",
            "       'avg_word_length1', 'avg_word_length2', 'unique_word_count',\n",
            "       'similar_word_count', 'fuzzy_word_partial_ratio', 'token_set_ratio',\n",
            "       'token_sort_ratio', 'word_overlap', 'jaccard_similarity',\n",
            "       'levenshtein_distance', 'length_ratio', 'common_2grams',\n",
            "       'common_3grams', 'average_word_frequency1', 'average_word_frequency2',\n",
            "       'average_word_frequency_diff', 'is_duplicate', 'question1_token',\n",
            "       'question2_token'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(train_df.shape)\n",
        "print(test_df.shape)\n",
        "print(train_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHAm8xMAbL11"
      },
      "outputs": [],
      "source": [
        "# Dropping Unnecessary Columns\n",
        "drop_columns =['clean_text1','clean_text2']\n",
        "train_df.drop(drop_columns,axis=1,inplace=True)\n",
        "test_df.drop(drop_columns,axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZTb3ScRMsyC",
        "outputId": "097681f3-e631-4ba6-c064-6bd10838523e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['percentage_common_tokens', 'question1_length', 'question2_length',\n",
              "       'length_difference', 'num_capital_letters1', 'num_capital_letters2',\n",
              "       'num_question_marks1', 'num_question_marks2', 'starts_with_are',\n",
              "       'starts_with_can', 'starts_with_how', 'word_count1', 'word_count2',\n",
              "       'sentence_count1', 'sentence_count2', 'avg_word_length1',\n",
              "       'avg_word_length2', 'unique_word_count', 'similar_word_count',\n",
              "       'fuzzy_word_partial_ratio', 'token_set_ratio', 'token_sort_ratio',\n",
              "       'word_overlap', 'jaccard_similarity', 'levenshtein_distance',\n",
              "       'length_ratio', 'common_2grams', 'common_3grams',\n",
              "       'average_word_frequency1', 'average_word_frequency2',\n",
              "       'average_word_frequency_diff', 'is_duplicate', 'question1_token',\n",
              "       'question2_token'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIC8ndniMpz3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2kzf7WebNze"
      },
      "outputs": [],
      "source": [
        "tokens = train_df['question1_token'].tolist() + train_df['question2_token'].tolist() + test_df['question1_token'].tolist() + test_df['question2_token'].tolist()\n",
        "word2vec_model = Word2Vec(tokens, window=5, vector_size=200,min_count=1, workers=4)\n",
        "\n",
        "# Function to calculate word embeddings for a sentence\n",
        "def embedding(tokens):\n",
        "    embeddings = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
        "    if len(embeddings) > 0:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(200)\n",
        "\n",
        "train_df['embedding_question1'] = train_df['question1_token'].apply(embedding)\n",
        "train_df['embedding_question2'] = train_df['question2_token'].apply(embedding)\n",
        "\n",
        "\n",
        "test_df['embedding_question1'] = test_df['question1_token'].apply(embedding)\n",
        "test_df['embedding_question2'] = test_df['question2_token'].apply(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxwVygw7kC2g"
      },
      "outputs": [],
      "source": [
        "def cos_similarity(row):\n",
        "    embedding1 = row['embedding_question1']\n",
        "    embedding2 = row['embedding_question2']\n",
        "    similarity_score = cosine_similarity([embedding1], [embedding2])[0][0]\n",
        "    return similarity_score\n",
        "\n",
        "train_df['cos_similarity'] = train_df.apply(cos_similarity, axis=1)\n",
        "test_df['cos_similarity'] = test_df.apply(cos_similarity, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAo2NQGwdgar"
      },
      "outputs": [],
      "source": [
        "y_train = train_df['is_duplicate']\n",
        "y_test = test_df['is_duplicate']\n",
        "\n",
        "columns_to_exclude = ['embedding_question1', 'embedding_question2']  # Replace with the actual column names\n",
        "X_train = train_df.drop(['is_duplicate','question1_token','question2_token'], axis=1)\n",
        "X_test = test_df.drop(['is_duplicate','question1_token','question2_token'],axis=1)\n",
        "# Separate columns for feature scaling\n",
        "columns_to_scale = [col for col in X_train.columns if col not in columns_to_exclude]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "\n",
        "train_quest1vec = pd.DataFrame(X_train['embedding_question1'].tolist(), columns=[f'q1_{i}' for i in range(200)])\n",
        "train_quest2vec = pd.DataFrame(X_train['embedding_question2'].tolist(), columns=[f'q2_{i}' for i in range(200)])\n",
        "\n",
        "test_quest1vec = pd.DataFrame(X_test['embedding_question1'].tolist(), columns=[f'q1_{i}' for i in range(200)])\n",
        "test_quest2vec = pd.DataFrame(X_test['embedding_question2'].tolist(), columns=[f'q2_{i}' for i in range(200)])\n",
        "\n",
        "train_scaled_columns = scaler.fit_transform(X_train[columns_to_scale])\n",
        "test_scaled_columns = scaler.transform(X_test[columns_to_scale])\n",
        "train_scaled_data = pd.DataFrame(train_scaled_columns, columns=[f'scaled_{i}' for i in range(len(columns_to_scale))])\n",
        "test_scaled_data = pd.DataFrame(test_scaled_columns, columns=[f'scaled_{i}' for i in range(len(columns_to_scale))])\n",
        "\n",
        "X_train = pd.concat([train_quest1vec, train_quest2vec, train_scaled_data], axis=1)\n",
        "X_test = pd.concat([test_quest1vec, test_quest2vec, test_scaled_data], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WukEdkNn8_Zi"
      },
      "outputs": [],
      "source": [
        "# Reset the indices of X_train and y_train\n",
        "trainx = X_train.reset_index(drop=True)\n",
        "trainy = y_train.reset_index(drop=True)\n",
        "\n",
        "# Concatenate X_train with y_train\n",
        "train_df_2 = pd.concat([trainx, trainy], axis=1)\n",
        "\n",
        "# Reset the indices of X_test and y_test\n",
        "testx = X_test.reset_index(drop=True)\n",
        "testy = y_test.reset_index(drop=True)\n",
        "\n",
        "# Concatenate X_test with y_test\n",
        "test_df_2 = pd.concat([testx, testy], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S0r7mTr9u4t",
        "outputId": "8c514fb0-d203-432e-d509-0207965a6518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(323432, 433)\n",
            "(80858, 433)\n"
          ]
        }
      ],
      "source": [
        "print(train_df_2.shape)\n",
        "print(test_df_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owq5wS2m9WbT"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/quora')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eH0XOTP9hFa"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/quora/trainembeddings.csv'\n",
        "test_path = '/content/drive/MyDrive/quora/testembeddings.csv'\n",
        "train_df_2.to_csv(train_path, index=False)\n",
        "test_df_2.to_csv(test_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYCSIyDd6-zE",
        "outputId": "5cb3de36-738c-4b16-a69a-7ce4ec0c362b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(323432, 432)\n",
            "(80858, 432)\n",
            "Index(['q1_0', 'q1_1', 'q1_2', 'q1_3', 'q1_4', 'q1_5', 'q1_6', 'q1_7', 'q1_8',\n",
            "       'q1_9',\n",
            "       ...\n",
            "       'scaled_22', 'scaled_23', 'scaled_24', 'scaled_25', 'scaled_26',\n",
            "       'scaled_27', 'scaled_28', 'scaled_29', 'scaled_30', 'scaled_31'],\n",
            "      dtype='object', length=432)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_train.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdIm24h8P-dt",
        "outputId": "e97cbde1-d3c2-48f5-811f-b223b89fb9d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['percentage_common_tokens',\n",
              " 'question1_length',\n",
              " 'question2_length',\n",
              " 'length_difference',\n",
              " 'num_capital_letters1',\n",
              " 'num_capital_letters2',\n",
              " 'num_question_marks1',\n",
              " 'num_question_marks2',\n",
              " 'starts_with_are',\n",
              " 'starts_with_can',\n",
              " 'starts_with_how',\n",
              " 'word_count1',\n",
              " 'word_count2',\n",
              " 'sentence_count1',\n",
              " 'sentence_count2',\n",
              " 'avg_word_length1',\n",
              " 'avg_word_length2',\n",
              " 'unique_word_count',\n",
              " 'similar_word_count',\n",
              " 'fuzzy_word_partial_ratio',\n",
              " 'token_set_ratio',\n",
              " 'token_sort_ratio',\n",
              " 'word_overlap',\n",
              " 'jaccard_similarity',\n",
              " 'levenshtein_distance',\n",
              " 'length_ratio',\n",
              " 'common_2grams',\n",
              " 'common_3grams',\n",
              " 'average_word_frequency1',\n",
              " 'average_word_frequency2',\n",
              " 'average_word_frequency_diff',\n",
              " 'cos_similarity']"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columns_to_scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sozG8nr8C-AW"
      },
      "outputs": [],
      "source": [
        "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
        "\n",
        "mlflow.set_experiment(\"Quora Question Pair Simmilarity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZHP2KK_DhND"
      },
      "outputs": [],
      "source": [
        "frofrom pickle import dump\n",
        "m pickle import dump\n",
        "\n",
        "dump(scaler, open('pickle_files/scaler.pkl', 'wb'))\n",
        "dump(word2vec_model, open('pickle_files/word2vec_model.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3lZMmlTfGge",
        "outputId": "a3db0758-4095-4774-9a12-02fd09f12f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Loss Test: 0.40095234222839193\n",
            "Log Loss Train: 0.3889106603662133\n",
            "Classification Report Test\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84     50803\n",
            "           1       0.73      0.73      0.73     30055\n",
            "\n",
            "    accuracy                           0.80     80858\n",
            "   macro avg       0.79      0.79      0.79     80858\n",
            "weighted avg       0.80      0.80      0.80     80858\n",
            "\n",
            "******************************************************\n",
            "Classification Report Train\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85    204224\n",
            "           1       0.74      0.74      0.74    119208\n",
            "\n",
            "    accuracy                           0.81    323432\n",
            "   macro avg       0.80      0.80      0.80    323432\n",
            "weighted avg       0.81      0.81      0.81    323432\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "\n",
        "# Initialize the LightGBM model\n",
        "lgm = LGBMClassifier()\n",
        "\n",
        "# Fit the model on the training data\n",
        "lgm.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_test_proba1 = lgm.predict_proba(X_test)\n",
        "y_pred_train_proba1 = lgm.predict_proba(X_train)\n",
        "\n",
        "# # Predict classes on the test set\n",
        "y_test_pred1 = lgm.predict(X_test)\n",
        "y_train_pred1 = lgm.predict(X_train)\n",
        "\n",
        "# Calculate log loss\n",
        "log_loss_test_score1 = log_loss(y_test, y_pred_test_proba1)\n",
        "log_loss_train_score1 = log_loss(y_train, y_pred_train_proba1)\n",
        "\n",
        "# Generate the classification report\n",
        "f1score_test_1 = f1_score(y_test, y_test_pred1)\n",
        "f1score_train_1 = f1_score(y_train, y_train_pred1)\n",
        "\n",
        "accuracy_test_1 = accuracy_score(y_test, y_test_pred1)\n",
        "accuracy_train_1 = accuracy_score(y_train, y_train_pred1)\n",
        "\n",
        "classificationtest = classification_report(y_test, y_test_pred1)\n",
        "classificationtrain = classification_report(y_train, y_train_pred1)\n",
        "\n",
        "print('Log Loss Test:',log_loss_test_score1)\n",
        "print('Log Loss Train:',log_loss_train_score1)\n",
        "\n",
        "print('Classification Report Test\\n', classificationtest)\n",
        "print('******************************************************')\n",
        "print('Classification Report Train\\n', classificationtrain)\n",
        "  # mlflow.log_metric(\"Log Loss\", log_loss_test_score1)\n",
        "  # mlflow.log_metric(\"F1 Score\", f1score_test_1)\n",
        "  # mlflow.log_metric(\"Accuracy Score\", accuracy_test_1)\n",
        "  # mlflow.sklearn.log_model(lgm, artifact_path=\"models\")\n",
        "  # mlflow.log_artifact(\"pickle_files/scaler.pkl\")\n",
        "  # mlflow.log_artifact(\"pickle_files/word2vec_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import dump\n",
        "dump(lgm, open('lgm.pkl', 'wb'))\n"
      ],
      "metadata": {
        "id": "jmH9IDGCmgIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump(naivebayes, open('naive.pkl', 'wb'))\n"
      ],
      "metadata": {
        "id": "LCmlPai1nCEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jAIboUi1nEew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RadxMzntnHqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g9J60oXYmuVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7GTLyroePbB",
        "outputId": "874962e2-8993-44b4-d3bc-09569bcf1066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Loss Test: 6.92820950613943\n",
            "Log Loss Train: 6.976045602616546\n",
            "Classification Report Test\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.59      0.70     50803\n",
            "           1       0.54      0.81      0.65     30055\n",
            "\n",
            "    accuracy                           0.67     80858\n",
            "   macro avg       0.69      0.70      0.67     80858\n",
            "weighted avg       0.73      0.67      0.68     80858\n",
            "\n",
            "******************************************************\n",
            "Classification Report Train\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.59      0.70    204224\n",
            "           1       0.54      0.81      0.65    119208\n",
            "\n",
            "    accuracy                           0.67    323432\n",
            "   macro avg       0.69      0.70      0.67    323432\n",
            "weighted avg       0.73      0.67      0.68    323432\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# with mlflow.start_run():\n",
        "#   mlflow.set_tag(\"dev\", \"NIKAvengers\")\n",
        "#   mlflow.set_tag(\"algo\", \"Naive Bayes\")\n",
        "\n",
        "# Initialize the Naive Bayes model\n",
        "naivebayes = GaussianNB()\n",
        "\n",
        "# Fit the model on the training data\n",
        "naivebayes.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_test_proba2 = naivebayes.predict_proba(X_test)\n",
        "y_pred_train_proba2 = naivebayes.predict_proba(X_train)\n",
        "\n",
        "# Predict classes on the test set\n",
        "y_test_pred2 = naivebayes.predict(X_test)\n",
        "y_train_pred2 = naivebayes.predict(X_train)\n",
        "\n",
        "# Calculate log loss\n",
        "log_loss_test_score2 = log_loss(y_test, y_pred_test_proba2)\n",
        "log_loss_train_score2 = log_loss(y_train, y_pred_train_proba2)\n",
        "\n",
        "# Generate the classification report\n",
        "f1score_test_2 = f1_score(y_test, y_test_pred2)\n",
        "f1score_train_2 = f1_score(y_train, y_train_pred2)\n",
        "\n",
        "accuracy_test_2 = accuracy_score(y_test, y_test_pred2)\n",
        "accuracy_train_2 = accuracy_score(y_train, y_train_pred2)\n",
        "\n",
        "classificationtest2 = classification_report(y_test, y_test_pred2)\n",
        "classificationtrain2 = classification_report(y_train, y_train_pred2)\n",
        "\n",
        "print('Log Loss Test:',log_loss_test_score2)\n",
        "print('Log Loss Train:',log_loss_train_score2)\n",
        "\n",
        "print('Classification Report Test\\n', classificationtest2)\n",
        "print('******************************************************')\n",
        "print('Classification Report Train\\n', classificationtrain2)\n",
        "\n",
        "  # mlflow.log_metric(\"Log Loss\", log_loss_test_score2)\n",
        "  # mlflow.log_metric(\"F1 Score\", f1score_test_2)\n",
        "  # mlflow.log_metric(\"Accuracy Score\", accuracy_test_2)\n",
        "  # mlflow.sklearn.log_model(naivebayes, artifact_path=\"models\")\n",
        "  # mlflow.log_artifact(\"pickle_files/scaler.pkl\")\n",
        "  # mlflow.log_artifact(\"pickle_files/word2vec_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPopeTEsp7yW",
        "outputId": "85330fb0-ecec-49e6-f5d7-f741e53dff40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log Loss Test: 0.3787669353437042\n",
            "Log Loss Train: 0.3301877660356633\n",
            "Classification Report Test\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85     50803\n",
            "           1       0.75      0.74      0.75     30055\n",
            "\n",
            "    accuracy                           0.81     80858\n",
            "   macro avg       0.80      0.80      0.80     80858\n",
            "weighted avg       0.81      0.81      0.81     80858\n",
            "\n",
            "******************************************************\n",
            "Classification Report Train\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.89      0.88    204224\n",
            "           1       0.80      0.78      0.79    119208\n",
            "\n",
            "    accuracy                           0.85    323432\n",
            "   macro avg       0.84      0.84      0.84    323432\n",
            "weighted avg       0.85      0.85      0.85    323432\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# with mlflow.start_run():\n",
        "#   mlflow.set_tag(\"dev\", \"NIKAvengers\")\n",
        "#   mlflow.set_tag(\"algo\", \"XGBoost\")\n",
        "#   # Initialize the XGBoost model\n",
        "xg_boost = xgb.XGBClassifier()\n",
        "\n",
        "# Fit the model on the training data\n",
        "xg_boost.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_test_proba3 = xg_boost.predict_proba(X_test)\n",
        "y_pred_train_proba3 = xg_boost.predict_proba(X_train)\n",
        "\n",
        "# Predict classes on the test set\n",
        "y_test_pred3 = xg_boost.predict(X_test)\n",
        "y_train_pred3 = xg_boost.predict(X_train)\n",
        "\n",
        "# Calculate log loss\n",
        "log_loss_test_score3 = log_loss(y_test, y_pred_test_proba3)\n",
        "log_loss_train_score3 = log_loss(y_train, y_pred_train_proba3)\n",
        "\n",
        "# Generate the classification report\n",
        "f1score_test_3 = f1_score(y_test, y_test_pred3)\n",
        "f1score_train_3 = f1_score(y_train, y_train_pred3)\n",
        "\n",
        "accuracy_test_3 = accuracy_score(y_test, y_test_pred3)\n",
        "accuracy_train_3 = accuracy_score(y_train, y_train_pred3)\n",
        "\n",
        "classificationtest3 = classification_report(y_test, y_test_pred3)\n",
        "classificationtrain3 = classification_report(y_train, y_train_pred3)\n",
        "\n",
        "print('Log Loss Test:',log_loss_test_score3)\n",
        "print('Log Loss Train:',log_loss_train_score3)\n",
        "\n",
        "print('Classification Report Test\\n', classificationtest3)\n",
        "print('******************************************************')\n",
        "print('Classification Report Train\\n', classificationtrain3)\n",
        "\n",
        "\n",
        "\n",
        "  # mlflow.log_metric(\"Log Loss\", log_loss_test_score3)\n",
        "  # mlflow.log_metric(\"F1 Score\", f1score_test_3)\n",
        "  # mlflow.log_metric(\"Accuracy Score\", accuracy_test_3)\n",
        "  # mlflow.sklearn.log_model(xg_boost, artifact_path=\"models\")\n",
        "  # mlflow.log_artifact(\"pickle_files/scaler.pkl\")\n",
        "  # mlflow.log_artifact(\"pickle_files/word2vec_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOX8SDvyJOj-",
        "outputId": "e369b92a-c90a-4cfc-d676-977416de7a18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'boosting_type': 'gbdt',\n",
              " 'class_weight': None,\n",
              " 'colsample_bytree': 1.0,\n",
              " 'importance_type': 'split',\n",
              " 'learning_rate': 0.1,\n",
              " 'max_depth': -1,\n",
              " 'min_child_samples': 20,\n",
              " 'min_child_weight': 0.001,\n",
              " 'min_split_gain': 0.0,\n",
              " 'n_estimators': 100,\n",
              " 'n_jobs': -1,\n",
              " 'num_leaves': 31,\n",
              " 'objective': None,\n",
              " 'random_state': None,\n",
              " 'reg_alpha': 0.0,\n",
              " 'reg_lambda': 0.0,\n",
              " 'silent': 'warn',\n",
              " 'subsample': 1.0,\n",
              " 'subsample_for_bin': 200000,\n",
              " 'subsample_freq': 0}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lgm.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3Urm2TYOqd-",
        "outputId": "6e63fdbe-03a6-4ed2-f262-2ddee1ffa5cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'objective': 'binary:logistic',\n",
              " 'use_label_encoder': None,\n",
              " 'base_score': None,\n",
              " 'booster': None,\n",
              " 'callbacks': None,\n",
              " 'colsample_bylevel': None,\n",
              " 'colsample_bynode': None,\n",
              " 'colsample_bytree': None,\n",
              " 'early_stopping_rounds': None,\n",
              " 'enable_categorical': False,\n",
              " 'eval_metric': None,\n",
              " 'feature_types': None,\n",
              " 'gamma': None,\n",
              " 'gpu_id': None,\n",
              " 'grow_policy': None,\n",
              " 'importance_type': None,\n",
              " 'interaction_constraints': None,\n",
              " 'learning_rate': None,\n",
              " 'max_bin': None,\n",
              " 'max_cat_threshold': None,\n",
              " 'max_cat_to_onehot': None,\n",
              " 'max_delta_step': None,\n",
              " 'max_depth': None,\n",
              " 'max_leaves': None,\n",
              " 'min_child_weight': None,\n",
              " 'missing': nan,\n",
              " 'monotone_constraints': None,\n",
              " 'n_estimators': 100,\n",
              " 'n_jobs': None,\n",
              " 'num_parallel_tree': None,\n",
              " 'predictor': None,\n",
              " 'random_state': None,\n",
              " 'reg_alpha': None,\n",
              " 'reg_lambda': None,\n",
              " 'sampling_method': None,\n",
              " 'scale_pos_weight': None,\n",
              " 'subsample': None,\n",
              " 'tree_method': None,\n",
              " 'validate_parameters': None,\n",
              " 'verbosity': None}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xg_boost.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6PTImrNqgY9",
        "outputId": "14c2ed1a-9721-48dd-f183-38fe62262e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Loss Test: 0.381077970134801\n",
            "Log Loss Train: 0.10986421818094379\n",
            "Classification Report Test\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87     50803\n",
            "           1       0.80      0.72      0.76     30055\n",
            "\n",
            "    accuracy                           0.83     80858\n",
            "   macro avg       0.82      0.81      0.81     80858\n",
            "weighted avg       0.83      0.83      0.83     80858\n",
            "\n",
            "******************************************************\n",
            "Classification Report Train\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    204224\n",
            "           1       1.00      1.00      1.00    119208\n",
            "\n",
            "    accuracy                           1.00    323432\n",
            "   macro avg       1.00      1.00      1.00    323432\n",
            "weighted avg       1.00      1.00      1.00    323432\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "  # Initialize the Random Forest model\n",
        "randomforest = RandomForestClassifier()\n",
        "\n",
        "  # Fit the model on the training data\n",
        "randomforest.fit(X_train, y_train)\n",
        "\n",
        "  # Predict probabilities on the test set\n",
        "y_pred_test_proba4 = randomforest.predict_proba(X_test)\n",
        "y_pred_train_proba4 = randomforest.predict_proba(X_train)\n",
        "\n",
        "  # Predict classes on the test set\n",
        "y_test_pred4 = randomforest.predict(X_test)\n",
        "y_train_pred4 = randomforest.predict(X_train)\n",
        "\n",
        "  # Calculate log loss\n",
        "log_loss_test_score4 = log_loss(y_test, y_pred_test_proba4)\n",
        "log_loss_train_score4 = log_loss(y_train, y_pred_train_proba4)\n",
        "\n",
        "  # Generate the classification report\n",
        "f1score_test_4 = f1_score(y_test, y_test_pred4)\n",
        "f1score_train_4 = f1_score(y_train, y_train_pred4)\n",
        "\n",
        "accuracy_test_4 = accuracy_score(y_test, y_test_pred4)\n",
        "accuracy_train_4 = accuracy_score(y_train, y_train_pred4)\n",
        "\n",
        "\n",
        "classificationtest4= classification_report(y_test, y_test_pred4)\n",
        "classificationtrain4= classification_report(y_train, y_train_pred4)\n",
        "\n",
        "print('Log Loss Test:',log_loss_test_score4)\n",
        "print('Log Loss Train:',log_loss_train_score4)\n",
        "\n",
        "print('Classification Report Test\\n', classificationtest4)\n",
        "print('******************************************************')\n",
        "print('Classification Report Train\\n', classificationtrain4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSHYqz0fxlLJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "with mlflow.start_run():\n",
        "    mlflow.set_tag(\"dev\", \"NIKAvengers\")\n",
        "    mlflow.set_tag(\"algo\", \"GradientBoostingClassifier\")\n",
        "\n",
        "    # Initialize the Gradient Boosting model\n",
        "    gradientboosting = GradientBoostingClassifier()\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    gradientboosting.fit(X_train, y_train)\n",
        "\n",
        "    # Predict probabilities on the test set\n",
        "    y_pred_test_proba5 = gradientboosting.predict_proba(X_test)\n",
        "    y_pred_train_proba5 = gradientboosting.predict_proba(X_train)\n",
        "\n",
        "    # Predict classes on the test set\n",
        "    y_test_pred5 = gradientboosting.predict(X_test)\n",
        "    y_train_pred5 = gradientboosting.predict(X_train)\n",
        "\n",
        "    # Calculate log loss\n",
        "    log_loss_test_score5 = log_loss(y_test, y_pred_test_proba5)\n",
        "    log_loss_train_score5 = log_loss(y_train, y_pred_train_proba5)\n",
        "\n",
        "    # Generate the classification report\n",
        "    f1score_test_5 = f1_score(y_test, y_test_pred5)\n",
        "    f1score_train_5 = f1_score(y_train, y_train_pred5)\n",
        "\n",
        "    accuracy_test_5 = accuracy_score(y_test, y_test_pred5)\n",
        "    accuracy_train_5 = accuracy_score(y_train, y_train_pred5)\n",
        "\n",
        "    mlflow.log_metric(\"Log Loss\", log_loss_test_score5)\n",
        "    mlflow.log_metric(\"F1 Score\", f1score_test_5)\n",
        "    mlflow.log_metric(\"Accuracy Score\", accuracy_test_5)\n",
        "    mlflow.sklearn.log_model(gradientboosting, artifact_path=\"models\")\n",
        "    mlflow.log_artifact(\"pickle_files/scaler.pkl\")\n",
        "    mlflow.log_artifact(\"pickle_files/word2vec_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dEqtF5wsFyA"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Use GPU-enabled version of XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Set the GPU-related parameters for LightGBM\n",
        "lgb_params = {\n",
        "    'device': 'gpu'\n",
        "}\n",
        "\n",
        "# Define the base models\n",
        "base_models = [\n",
        "    ('lgm', lgb.LGBMClassifier()),\n",
        "    ('xgb', XGBClassifier(tree_method='gpu_hist')),\n",
        "    ('randomforest', RandomForestClassifier())\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ieUBiw2Xser0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Train the stacking classifier\n",
        "stacking_classifier = StackingClassifier(estimators=base_models, final_estimator=XGBClassifier(tree_method='gpu_hist'))\n",
        "stacking_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = stacking_classifier.predict(X_test)\n",
        "y_pred_train = stacking_classifier.predict(X_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_proba = stacking_classifier.predict_proba(X_test)\n",
        "y_pred_proba_train = stacking_classifier.predict_proba(X_train)\n",
        "\n",
        "# Calculate log loss\n",
        "log_loss_score = log_loss(y_test, y_pred_proba)\n",
        "log_loss_score_train = log_loss(y_train, y_pred_proba_train)\n",
        "\n",
        "# Generate the classification report\n",
        "report_test = classification_report(y_test, y_pred)\n",
        "report_train = classification_report(y_train, y_pred_train)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1score = f1_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OfrwMFmxtD-9",
        "outputId": "c9d79956-58f4-4adc-ad8e-b3d06b9df9fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log loss (test): 0.3484\n",
            "Log loss (train): 0.0839\n",
            "\n",
            "Classification Report (test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87     50803\n",
            "           1       0.79      0.74      0.77     30055\n",
            "\n",
            "    accuracy                           0.83     80858\n",
            "   macro avg       0.82      0.81      0.82     80858\n",
            "weighted avg       0.83      0.83      0.83     80858\n",
            "\n",
            "\n",
            "Classification Report (train):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99    204224\n",
            "           1       0.98      1.00      0.99    119208\n",
            "\n",
            "    accuracy                           0.99    323432\n",
            "   macro avg       0.99      0.99      0.99    323432\n",
            "weighted avg       0.99      0.99      0.99    323432\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print the log loss\n",
        "print('Log loss (test): {:.4f}'.format(log_loss_score))\n",
        "print('Log loss (train): {:.4f}'.format(log_loss_score_train))\n",
        "print()\n",
        "\n",
        "# Print the classification reports\n",
        "print('Classification Report (test):')\n",
        "print(report_test)\n",
        "print()\n",
        "\n",
        "print('Classification Report (train):')\n",
        "print(report_train)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJK1ZuRrxzXo0A1XXzhb1n"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}